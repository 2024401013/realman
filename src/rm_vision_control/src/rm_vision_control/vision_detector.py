#!/usr/bin/env python

# src/rm_vision_control/scripts/vision_detector.py
import os
os.environ["ORT_DISABLE_OPSET_VALIDATION"] = "1"
os.environ["ORT_DISABLE_ONNX_VERSION_CHECK"] = "1"
os.environ["ORT_DISABLE_AFFINITY"] = "1"

import rospy
import cv2
import torch
import numpy as np
import json
import tf  
from datetime import datetime
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import onnxruntime as ort
import threading
import time


class ONNXDetector:
    """ONNX æ¨¡å‹æ¨ç†å™¨ï¼ˆé€‚é… Opset 20 + IR 9 + Jetsonï¼‰"""
    def __init__(self, model_path, conf_threshold=0.25, iou_threshold=0.45):
        self.model_path = model_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.session = None
        self.input_name = None
        self.output_name = None
        self.input_shape = None
        self.input_size = 640  # é»˜è®¤å°ºå¯¸ï¼ŒåŠ è½½æ¨¡å‹åæ›´æ–°
        self.load_model()
    
    def load_model(self):
        try:
            rospy.loginfo(f"ğŸ” å®é™… ORT ç‰ˆæœ¬: {ort.__version__}")
            # é…ç½® Session
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            sess_options.intra_op_num_threads = 4
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            self.session = ort.InferenceSession(
                self.model_path,
                sess_options=sess_options,
                providers=providers
            )

            # ====== åŠ¨æ€è·å–è¾“å…¥å°ºå¯¸ ======
            input_shape = self.session.get_inputs()[0].shape
            if len(input_shape) == 4:
                self.input_size = input_shape[2]  # height
                rospy.loginfo(f"â„¹ï¸ æ¨¡å‹è¾“å…¥å°ºå¯¸: {self.input_size}x{self.input_size}")
            else:
                rospy.logwarn(f"âš ï¸ æœªçŸ¥è¾“å…¥å½¢çŠ¶: {input_shape}, ä½¿ç”¨é»˜è®¤ 640")
                self.input_size = 640
            # ===========================

            # ====== æ¨¡å‹æ¨ç†éªŒè¯ ======
            dummy = np.random.randn(1, 3, self.input_size, self.input_size).astype(np.float32)
            self.session.run(None, {self.session.get_inputs()[0].name: dummy})
            rospy.loginfo("âœ… æ¨¡å‹æ¨ç†éªŒè¯é€šè¿‡")
            # ========================

            self.input_name = self.session.get_inputs()[0].name
            self.output_name = self.session.get_outputs()[0].name
            rospy.loginfo(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")       

        except Exception as e:
            rospy.logerr(f"âŒ æ¨¡å‹åŠ è½½/éªŒè¯å¤±è´¥: {e}")
            self.session = None
            self.input_size = 640  

    
    def preprocess(self, image):
        """å›¾åƒé¢„å¤„ç†ï¼ˆè‡ªåŠ¨é€‚é…æ¨¡å‹è¾“å…¥å°ºå¯¸ï¼‰"""
        # è°ƒæ•´å¤§å°ï¼ˆä½¿ç”¨æ¨¡å‹è¾“å…¥å°ºå¯¸ï¼‰
        img_resized = cv2.resize(image, (self.input_size, self.input_size))
        
        # BGR -> RGB
        img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)
        
        # å½’ä¸€åŒ–
        img_normalized = img_rgb.astype(np.float32) / 255.0
        
        # HWC -> CHW
        img_chw = img_normalized.transpose(2, 0, 1)
        
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        img_batch = np.expand_dims(img_chw, axis=0)
        
        return img_batch
    
    def postprocess(self, outputs, original_shape, conf_thresh=None):
        """åå¤„ç†è¾“å‡º"""
        if conf_thresh is None:
            conf_thresh = self.conf_threshold
        
        if len(outputs) == 0 or outputs[0] is None:
            return []
        
        predictions = outputs[0][0]  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼Œå½¢çŠ¶ä¸º [num_predictions, 6]
        
        if len(predictions) == 0:
            return []
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦
        if predictions.shape[1] >= 5:
            mask = predictions[:, 4] > conf_thresh
            predictions = predictions[mask]
        
        results = []
        h, w = original_shape[:2]
        
        for pred in predictions:
            if len(pred) >= 6:
                x1, y1, x2, y2, conf, cls = pred[:6]
                
                # åå½’ä¸€åŒ–åˆ°åŸå§‹å›¾åƒå°ºå¯¸
                x1 = int(x1 * w / self.input_size)
                y1 = int(y1 * h / self.input_size)
                x2 = int(x2 * w / self.input_size)
                y2 = int(y2 * h / self.input_size)
                
                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                x1 = max(0, min(x1, w))
                y1 = max(0, min(y1, h))
                x2 = max(0, min(x2, w))
                y2 = max(0, min(y2, h))
                
                # è®¡ç®—ä¸­å¿ƒç‚¹
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2
                
                results.append({
                    'bbox': [float(x1), float(y1), float(x2), float(y2)],
                    'center': [center_x, center_y],
                    'confidence': float(conf),
                    'class_id': int(cls),
                    'class_name': f'class{int(cls)}'
                })
        
        return results
    
    def detect(self, image, conf_threshold=None):
        """æ‰§è¡Œæ£€æµ‹"""
        if self.session is None:
            rospy.logwarn("ONNX æ¨¡å‹æœªåŠ è½½")
            return []
        
        if conf_threshold is None:
            conf_threshold = self.conf_threshold
        
        # è®°å½•æ¨ç†æ—¶é—´
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†ï¼ˆè‡ªåŠ¨é€‚é…å°ºå¯¸ï¼‰
            img_tensor = self.preprocess(image)
            
            # æ¨ç†
            outputs = self.session.run([self.output_name], {self.input_name: img_tensor})
            
            # åå¤„ç†
            results = self.postprocess(outputs, image.shape, conf_threshold)
            
            inference_time = (time.time() - start_time) * 1000
            
            if len(results) > 0:
                rospy.logdebug(f"æ¨ç†æ—¶é—´: {inference_time:.1f}ms, æ£€æµ‹åˆ° {len(results)} ä¸ªç›®æ ‡")
            
            return results
            
        except Exception as e:
            rospy.logerr(f"ONNX æ¨ç†å¤±è´¥: {e}")
            return []

class VisionDetector:
    def __init__(self):
        self.bridge = CvBridge()
        
        # ==== æ¨¡å‹è·¯å¾„ï¼ˆæ”¹ä¸ºè½¬æ¢åçš„ Opset 20 + IR 9 ç‰ˆæœ¬ï¼‰====
        self.crack_model_path = '/home/nvidia/rm_robot/src/rm_vision_control/models/crack_model.onnx' 
        self.target_model_path = '/home/nvidia/rm_robot/src/rm_vision_control/models/target_model.onnx' 
        
        # ==== ç›¸æœºå†…å‚ ====
        self.camera_matrix = np.array([
            [643.464233398438, 0, 653.559936523438],
            [0, 642.685913085938, 402.54541015625],
            [0, 0, 1]
        ], dtype=np.float32)
        
        # ==== æ·±åº¦å¤„ç†å‚æ•° ====
        self.depth_scale = 0.001  # mm to m
        self.depth_roi_size = 15
        self.depth_min_threshold = 300
        self.depth_max_threshold = 8000
        
        # ==== ç›¸æœºåˆ°æœºæ¢°è‡‚æœ«ç«¯çš„å›ºå®šåç§» ====
        self.camera_to_link6_trans = np.array([0.0, -0.075, 0.028])
        self.camera_to_link6_rot = np.array([0.0, 0.0, 0.0, 1.0])
        
        # ==== TFç›‘å¬å™¨ ====
        self.tf_listener = tf.TransformListener()
        
        # ==== åŠ è½½ ONNX æ¨¡å‹ ====
        self.crack_detector = None
        self.target_detector = None
        self.load_onnx_models()
        
        # ==== å›¾åƒè®¢é˜… ====
        self.color_image = None
        self.depth_image = None
        self.image_lock = threading.Lock()
        
        # è®¢é˜…å›¾åƒè¯é¢˜
        rospy.Subscriber('/camera/color/image_raw', Image, self.color_callback)
        rospy.Subscriber('/camera/aligned_depth_to_color/image_raw', Image, self.depth_callback)
        
        # ==== æ£€æµ‹ç»“æœå¯è§†åŒ– ====
        self.visualization_enabled = rospy.get_param('~visualization', True)
        
        # ==== åˆ›å»ºç»“æœä¿å­˜ç›®å½• ====
        self.result_dir = "detection_results"
        os.makedirs(self.result_dir, exist_ok=True)
        
        rospy.loginfo("âœ… VisionDetector initialized with ONNX models")
    
    def color_callback(self, msg):
        """RGBå›¾åƒå›è°ƒ"""
        try:
            with self.image_lock:
                self.color_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
        except Exception as e:
            rospy.logerr(f"âŒ Error converting color image: {e}")
    
    def depth_callback(self, msg):
        """æ·±åº¦å›¾åƒå›è°ƒ"""
        try:
            with self.image_lock:
                self.depth_image = self.bridge.imgmsg_to_cv2(msg, 'passthrough')
        except Exception as e:
            rospy.logerr(f"âŒ Error converting depth image: {e}")
    
    def load_onnx_models(self):
        """åŠ è½½ ONNX æ¨¡å‹"""
        try:
            self.crack_detector = ONNXDetector(self.crack_model_path, conf_threshold=0.55)
            rospy.loginfo(f"âœ… Loaded crack ONNX model: {self.crack_model_path}")
        except Exception as e:
            rospy.logerr(f"âŒ Failed to load crack ONNX model: {e}")
            self.crack_detector = None
        
        try:
            self.target_detector = ONNXDetector(self.target_model_path, conf_threshold=0.3)
            rospy.loginfo(f"âœ… Loaded target ONNX model: {self.target_model_path}")
        except Exception as e:
            rospy.logerr(f"âŒ Failed to load target ONNX model: {e}")
            self.target_detector = None
    
    def calculate_robust_depth(self, depth_image, x, y, roi_size=10):
        """åœ¨æŒ‡å®šç‚¹å‘¨å›´è®¡ç®—é²æ£’çš„æ·±åº¦å€¼"""
        h, w = depth_image.shape
        
        # ç¡®å®šROIè¾¹ç•Œ
        x_min = max(0, int(x) - roi_size)
        x_max = min(w, int(x) + roi_size + 1)
        y_min = max(0, int(y) - roi_size)
        y_max = min(h, int(y) + roi_size + 1)
        
        # æå–ROIåŒºåŸŸ
        roi = depth_image[y_min:y_max, x_min:x_max]
        
        # è·å–æœ‰æ•ˆæ·±åº¦å€¼
        valid_mask = (roi > self.depth_min_threshold) & (roi < self.depth_max_threshold)
        valid_depths = roi[valid_mask]
        
        if len(valid_depths) == 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå€¼ï¼Œå°è¯•æ‰©å¤§ROI
            roi_size_large = roi_size * 2
            x_min = max(0, int(x) - roi_size_large)
            x_max = min(w, int(x) + roi_size_large + 1)
            y_min = max(0, int(y) - roi_size_large)
            y_max = min(h, int(y) + roi_size_large + 1)
            roi = depth_image[y_min:y_max, x_min:x_max]
            valid_mask = (roi > self.depth_min_threshold) & (roi < self.depth_max_threshold)
            valid_depths = roi[valid_mask]
            
            if len(valid_depths) == 0:
                rospy.logdebug(f"âŒ No valid depths at ({x:.1f}, {y:.1f})")
                return 0
        
        # ä½¿ç”¨ä¸­å€¼æ»¤æ³¢
        median_depth = np.median(valid_depths)
        return median_depth
    
    def pixel_to_3d_camera(self, u, v, depth_mm):
        """åƒç´ åæ ‡è½¬3Dåæ ‡ï¼ˆç›¸æœºåæ ‡ç³»ï¼‰"""
        if depth_mm <= 0:
            return None
        
        # è½¬æ¢ä¸ºç±³
        depth = depth_mm * self.depth_scale
        
        fx = self.camera_matrix[0, 0]
        fy = self.camera_matrix[1, 1]
        cx = self.camera_matrix[0, 2]
        cy = self.camera_matrix[1, 2]
        
        x = (u - cx) * depth / fx
        y = (v - cy) * depth / fy
        z = depth
        
        return np.array([x, y, z])
    
    def transform_to_robot(self, point_camera):
        """å°†ç›¸æœºåæ ‡ç³»ç‚¹è½¬æ¢åˆ°æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»"""
        if point_camera is None:
            return None
        
        try:
            # 1. è·å–link6åœ¨base_linkä¸‹çš„å®æ—¶ä½å§¿
            (link6_trans, link6_rot) = self.tf_listener.lookupTransform(
                "base_link", "link6", rospy.Time(0)
            )
            
            # 2. æ„å»ºã€Œç›¸æœºâ†’link6ã€çš„å˜æ¢çŸ©é˜µ
            camera_to_link6_mat = self.tf_listener.fromTranslationRotation(
                self.camera_to_link6_trans, self.camera_to_link6_rot
            )
            
            # 3. ç›¸æœºç³»ç‚¹è½¬é½æ¬¡åæ ‡
            point_camera_h = np.append(point_camera, 1)
            
            # 4. è½¬æ¢åˆ°link6ç³»
            point_link6_h = np.dot(camera_to_link6_mat, point_camera_h)
            point_link6 = point_link6_h[:3]
            
            # 5. æ„å»ºã€Œlink6â†’base_linkã€çš„å˜æ¢çŸ©é˜µ
            link6_to_base_mat = self.tf_listener.fromTranslationRotation(link6_trans, link6_rot)
            
            # 6. link6ç³»ç‚¹è½¬é½æ¬¡åæ ‡
            point_link6_h = np.append(point_link6, 1)
            
            # 7. è½¬æ¢åˆ°base_linkç³»
            point_base_h = np.dot(link6_to_base_mat, point_link6_h)
            point_base = point_base_h[:3]
            
            return point_base
        
        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException) as e:
            rospy.logwarn(f"âš ï¸ TF transform error: {e}")
            return None
        except Exception as e:
            rospy.logerr(f"âŒ Unexpected error in transform_to_robot: {e}")
            return None
    
    def visualize_detection(self, image, detections, model_name="detection"):
        """å¯è§†åŒ–æ£€æµ‹ç»“æœï¼ˆæ·»åŠ å¼‚å¸¸ä¿æŠ¤ï¼‰"""
        if not self.visualization_enabled or image is None:
            return image
        
        try:
            img_viz = image.copy()
            
            for det in detections:
                bbox = det.get('bbox', [])
                center = det.get('center', [0, 0])
                confidence = det.get('confidence', 0)
                class_name = det.get('class_name', 'object')
                
                if len(bbox) == 4:
                    x1, y1, x2, y2 = map(int, bbox)
                    
                    # ç»˜åˆ¶è¾¹ç•Œæ¡†
                    color = (0, 255, 0)  # ç»¿è‰²
                    cv2.rectangle(img_viz, (x1, y1), (x2, y2), color, 2)
                    
                    # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                    cv2.circle(img_viz, (center[0], center[1]), 5, (0, 0, 255), -1)
                    
                    # ç»˜åˆ¶æ ‡ç­¾
                    label = f"{class_name}: {confidence:.2f}"
                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                    cv2.rectangle(img_viz, (x1, y1 - label_size[1] - 10),
                                 (x1 + label_size[0], y1), color, -1)
                    cv2.putText(img_viz, label, (x1, y1 - 5),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # æ˜¾ç¤ºFPSæˆ–æ¨¡å‹åç§°
            cv2.putText(img_viz, f"Model: {model_name}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            
            return img_viz
        except Exception as e:
            rospy.logerr(f"âŒ Visualization error: {e}")
            return image
    
    def detect_cracks(self):
        """æ£€æµ‹è£‚ç¼"""
        if self.color_image is None or self.crack_detector is None:
            rospy.logdebug("âŒ No image or ONNX model for crack detection")
            return {'detected': False}
        
        # è·å–å›¾åƒå‰¯æœ¬
        with self.image_lock:
            color_img = self.color_image.copy()
            depth_img = self.depth_image.copy() if self.depth_image is not None else None
        
        try:
            # ä½¿ç”¨ ONNX æ¨¡å‹æ£€æµ‹
            detections = self.crack_detector.detect(color_img)
            
        except Exception as e:
            rospy.logerr(f"âŒ Crack detection error: {e}")
            return {'detected': False}
        
        if len(detections) > 0:
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            detections.sort(key=lambda x: x['confidence'], reverse=True)
            best_det = detections[0]
            
            bbox = best_det['bbox']
            confidence = best_det['confidence']
            center_x, center_y = best_det['center']
            
            # è·å–æ·±åº¦å€¼
            depth_mm = 0
            point_3d_camera = None
            point_3d_robot = None
            
            if depth_img is not None:
                depth_mm = self.calculate_robust_depth(depth_img, center_x, center_y, self.depth_roi_size)
                
                if depth_mm > 0:
                    # è½¬æ¢åˆ°3Dåæ ‡
                    point_3d_camera = self.pixel_to_3d_camera(center_x, center_y, depth_mm)
                    
                    if point_3d_camera is not None:
                        # è½¬æ¢åˆ°æœºæ¢°è‡‚åŸºåº§åæ ‡ç³»
                        point_3d_robot = self.transform_to_robot(point_3d_camera)
            
            # å¯è§†åŒ–
            annotated_image = self.visualize_detection(color_img, detections, "Crack")
            
            result = {
                'detected': True,
                'confidence': float(confidence),
                'bbox': bbox,
                'center_pixel': (center_x, center_y),
                'depth_mm': depth_mm,
                'depth_m': depth_mm * self.depth_scale if depth_mm > 0 else 0,
                'point_3d_camera': point_3d_camera.tolist() if point_3d_camera is not None else None,
                'point_3d_robot': point_3d_robot.tolist() if point_3d_robot is not None else None,
                'annotated_image': annotated_image,
                'detections': detections,
                'timestamp': rospy.Time.now().to_sec()
            }
            
            if depth_mm > 0:
                rospy.loginfo(f"âœ… Crack detected! Conf: {confidence:.2f}, Depth: {depth_mm/1000:.3f}m")
            else:
                rospy.loginfo(f"âœ… Crack detected! Conf: {confidence:.2f} (No valid depth)")
            
            return result
        
        return {'detected': False}
    
    def detect_target(self):
        """æ£€æµ‹ç›®æ ‡ï¼ˆ16ä¸ªåœ†ç‚¹çš„çŸ©å½¢ï¼‰"""
        if self.color_image is None or self.target_detector is None:
            rospy.logdebug("âŒ No image or ONNX model for target detection")
            return None
        
        # è·å–å›¾åƒå‰¯æœ¬
        with self.image_lock:
            color_img = self.color_image.copy()
            depth_img = self.depth_image.copy() if self.depth_image is not None else None
        
        try:
            detections = self.target_detector.detect(color_img)
            
        except Exception as e:
            rospy.logerr(f"âŒ Target detection error: {e}")
            return None
        
        if len(detections) > 0:
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹ç»“æœ
            detections.sort(key=lambda x: x['confidence'], reverse=True)
            best_det = detections[0]
            
            bbox = best_det['bbox']
            confidence = best_det['confidence']
            center_x, center_y = best_det['center']
            
            # è·å–æ·±åº¦å€¼
            depth_mm = 0
            point_3d_camera = None
            point_3d_robot = None
            
            if depth_img is not None:
                depth_mm = self.calculate_robust_depth(depth_img, center_x, center_y, self.depth_roi_size)
                
                if depth_mm > 0:
                    # è·å–ä¸­å¿ƒç‚¹çš„3Dåæ ‡
                    point_3d_camera = self.pixel_to_3d_camera(center_x, center_y, depth_mm)
                    
                    if point_3d_camera is not None:
                        point_3d_robot = self.transform_to_robot(point_3d_camera)
            
            # å¯è§†åŒ–
            annotated_image = self.visualize_detection(color_img, detections, "Target")
            
            result = {
                'center_2d': (center_x, center_y),
                'center_3d_robot': point_3d_robot.tolist() if point_3d_robot is not None else None,
                'center_3d_camera': point_3d_camera.tolist() if point_3d_camera is not None else None,
                'confidence': float(confidence),
                'depth_mm': depth_mm,
                'depth_m': depth_mm * self.depth_scale if depth_mm > 0 else 0,
                'bbox': bbox,
                'detections': detections,
                'annotated_image': annotated_image,
                'timestamp': rospy.Time.now().to_sec()
            }
            
            if point_3d_robot is not None:
                rospy.loginfo(f"âœ… Target detected! Center at ({center_x}, {center_y}), "
                             f"Depth: {depth_mm/1000:.3f}m, Robot: {point_3d_robot}")
            
            return result
        
        return None
    
    def calculate_16_points(self, center_point_robot, grid_size=0.05, rows=4, cols=4):
        """è®¡ç®—16ä¸ªç‚¹çš„3Dåæ ‡ï¼ˆè›‡å½¢é¡ºåºï¼‰"""
        points = []
        
        if center_point_robot is None or len(center_point_robot) < 3:
            return points
        
        # è®¡ç®—èµ·å§‹ç‚¹ï¼ˆå·¦ä¸Šè§’ï¼‰
        start_x = center_point_robot[0]  # xä¸å˜ï¼ˆæ·±åº¦æ–¹å‘ï¼‰
        start_y = center_point_robot[1] - (cols - 1) * grid_size / 2  # yèµ·å§‹ï¼ˆå‘å·¦åç§»ï¼‰
        start_z = center_point_robot[2] + (rows - 1) * grid_size / 2  # zèµ·å§‹ï¼ˆå‘ä¸Šåç§»ï¼‰
        
        for i in range(rows):
            row_z = start_z - i * grid_size  # ä»ä¸Šå¾€ä¸‹ï¼Œzé€’å‡
            for j in range(cols):
                # è›‡å½¢é¡ºåºï¼šå¥‡æ•°è¡Œä»å·¦åˆ°å³ï¼Œå¶æ•°è¡Œä»å³åˆ°å·¦
                if i % 2 == 0:  # å¶æ•°è¡Œï¼ˆç¬¬0,2è¡Œï¼‰ä»å·¦åˆ°å³
                    col_y = start_y + j * grid_size
                else:  # å¥‡æ•°è¡Œï¼ˆç¬¬1,3è¡Œï¼‰ä»å³åˆ°å·¦
                    col_y = start_y + (cols - 1 - j) * grid_size
                
                points.append((start_x, col_y, row_z))
        
        return points
    
    def save_detection_result(self, detection_result, save_image=True):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        
        # ä¿å­˜å›¾åƒ
        if save_image and 'annotated_image' in detection_result:
            image_path = os.path.join(self.result_dir, f"{timestamp}.jpg")
            try:
                cv2.imwrite(image_path, detection_result['annotated_image'])
                detection_result['image_path'] = image_path
                rospy.loginfo(f"ğŸ’¾ Image saved: {image_path}")
            except Exception as e:
                rospy.logerr(f"âŒ Failed to save image: {e}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'timestamp': detection_result.get('timestamp', rospy.Time.now().to_sec()),
            'detection_time': datetime.now().isoformat(),
            'detected': detection_result.get('detected', False),
            'confidence': detection_result.get('confidence', 0.0),
            'point_3d_robot': detection_result.get('point_3d_robot'),
            'point_3d_camera': detection_result.get('point_3d_camera'),
            'depth_mm': detection_result.get('depth_mm'),
            'depth_m': detection_result.get('depth_m'),
            'bbox': detection_result.get('bbox'),
            'center_pixel': detection_result.get('center_pixel'),
            'image_path': detection_result.get('image_path', ''),
            'model_type': 'ONNX'
        }
        
        # ä¿å­˜ä¸ºJSON
        json_path = os.path.join(self.result_dir, f"{timestamp}.json")
        try:
            with open(json_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            rospy.loginfo(f"ğŸ’¾ Detection result saved: {json_path}")
            return json_path
        except Exception as e:
            rospy.logerr(f"âŒ Failed to save JSON: {e}")
            return None
    
    def run_detection_loop(self, rate_hz=5):
        """è¿è¡Œæ£€æµ‹å¾ªç¯ï¼ˆæ·»åŠ å¯è§†åŒ–å¼‚å¸¸ä¿æŠ¤ï¼‰"""
        rate = rospy.Rate(rate_hz)
        
        while not rospy.is_shutdown():
            try:
                # æ£€æµ‹è£‚ç¼
                crack_result = self.detect_cracks()
                
                if crack_result.get('detected', False):
                    self.save_detection_result(crack_result, save_image=True)
                
                # æ£€æµ‹ç›®æ ‡
                target_result = self.detect_target()
                
                if target_result is not None:
                    rospy.loginfo(f"ğŸ¯ Target detected at {target_result['center_2d']}")
                    
                    # è®¡ç®—16ä¸ªç‚¹
                    if target_result['center_3d_robot'] is not None:
                        points_3d = self.calculate_16_points(target_result['center_3d_robot'])
                        rospy.loginfo(f"ğŸ“ Calculated 16 points from center")
                        
                        # è¿™é‡Œå¯ä»¥å‘å¸ƒåˆ°ROSè¯é¢˜æˆ–æ‰§è¡Œå…¶ä»–æ“ä½œ
                        # self.publish_target_points(points_3d)
                
                # æ˜¾ç¤ºå¯è§†åŒ–çª—å£ï¼ˆæ·»åŠ å¼‚å¸¸ä¿æŠ¤ï¼‰
                if self.visualization_enabled:
                    try:
                        if crack_result.get('detected', False):
                            cv2.imshow('Crack Detection', crack_result['annotated_image'])
                        
                        if target_result is not None:
                            cv2.imshow('Target Detection', target_result['annotated_image'])
                        
                        cv2.waitKey(1)
                    except Exception as e:
                        rospy.logwarn(f"âš ï¸ Visualization window error: {e}")
                
            except Exception as e:
                rospy.logerr(f"âŒ Error in detection loop: {e}")
            
            rate.sleep()
        
        # å…³é—­çª—å£
        try:
            cv2.destroyAllWindows()
        except:
            pass

if __name__ == "__main__":
    rospy.init_node("vision_detector_node")
    
    # è·å–å‚æ•°
    detection_rate = rospy.get_param('~detection_rate', 5)
    visualization = rospy.get_param('~visualization', True)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = VisionDetector()
    detector.visualization_enabled = visualization
    
    rospy.loginfo(f"ğŸš€ Starting vision detector with ONNX models")
    rospy.loginfo(f"   Detection rate: {detection_rate} Hz")
    rospy.loginfo(f"   Visualization: {visualization}")
    
    # è¿è¡Œæ£€æµ‹å¾ªç¯
    detector.run_detection_loop(rate_hz=detection_rate)
